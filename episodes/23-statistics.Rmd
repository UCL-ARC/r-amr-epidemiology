---
title: "Basic Statistics: describing, modelling and reporting"
teaching: 60
exercises: 20
source: Rmd
---

```{r, include=FALSE}
## Limit printing output to max. 100 lines to avoid endless scrolling
options(max.print = 100)
```


::::::::::::::::::::::::::::::::::::::: objectives

- To be able to describe the different types of data
- To be able to do basic data exploration of a dataset
- To be able to calculate descriptive statistics
- To be able to perform statistical inference on a dataset

::::::::::::::::::::::::::::::::::::::::::::::::::

:::::::::::::::::::::::::::::::::::::::: questions

- How can I detect the type of data I have?
- How can I make meaningful summaries of my data?

::::::::::::::::::::::::::::::::::::::::::::::::::

## Content

-   Types of Data
-   Exploring your dataset
-   Descriptive Statistics
-   Inferential Statistics

## Data

```{r libraries, message=FALSE, warning=FALSE}
# We will need these libraries and this data later.
library(tidyverse)
library(ggplot2)

# loading data
hgtwgt_survey <- read.csv("data/hgt_wgt.csv")

```

We are going to use synthetic data that has been generated based upon Health Survey for England, 2021 data tables. 

## The big picture

-   Research often seeks to answer a question about a larger population by collecting data on a small sample
-   Data collection:
    -   Many variables
    -   For each person/unit.
-   This procedure, *sampling*, must be controlled so as to ensure **representative** data.

## Descriptive and inferential statistics

::: callout
Just as data in general are of different types - for example numeric vs text data - statistical data are assigned to different *levels of measure*. The level of measure determines how we can describe and model the data.
:::

# Describing data

-   Continuous variables
-   Discrete variables

::: callout
How do we convey information on what your data looks like, using numbers or figures?
:::

### Describing continuous data.

First establish the distribution of the data. You can visualise this with a histogram.

```{r}
ggplot(hgtwgt_survey, aes(x = weight.kg.)) +
  geom_histogram()
```

What is the distribution of this data?

### What is the distribution of population?

If the raw values are difficult to visualise, so we can take the log of the values and log those.  Try this command

```{r include=TRUE}
ggplot(hgtwgt_survey, aes(x = log(weight.kg.))) +
  geom_histogram()
```

What is the distribution of this data?

## Parametric vs non-parametric analysis

-   Parametric analysis assumes that
    -   The data follows a known distribution
    -   It can be described using *parameters*
    -   Examples of distributions include, normal, Poisson, exponential.
-   Non parametric data
    -   The data can't be said to follow a known distribution

::::::::::::::::::::::::::::::::::::: instructor
Emphasise that parametric is not equal to normal.
::::::::::::::::::::::::::::::::::::::::::::::::

### Describing parametric and non-parametric data

How do you use numbers to convey what your data looks like.

-   Parametric data
    -   Use the parameters that describe the distribution.
    -   For a Gaussian (normal) distribution - use mean and standard deviation
    -   For a Poisson distribution - use average event rate
    -   etc.
-   Non Parametric data
    -   Use the median (the middle number when they are ranked from lowest to highest) and the interquartile range (the number 75% of the way up the list when ranked minus the number 25% of the way)
-   You can use the command `summary(data_frame_name)` to get these numbers for each variable.

## Mean versus standard deviation

-   What does standard deviation mean?
-   Both graphs have the same mean (center), but the second one has data which is more spread out.

```{r}
# small standard deviation
dummy_1 <- rnorm(1000, mean = 10, sd = 0.5)
dummy_1 <- as.data.frame(dummy_1)
ggplot(dummy_1, aes(x = dummy_1)) +
  geom_histogram()

# larger standard deviation
dummy_2 <- rnorm(1000, mean = 10, sd = 200)
dummy_2 <- as.data.frame(dummy_2)
ggplot(dummy_2, aes(x = dummy_2)) +
  geom_histogram()
```

::::::::::::::::::::::::::::::::::::: instructor
Get them to plot the graphs. Explain that we are generating random data from different distributions and plotting them.
::::::::::::::::::::::::::::::::::::::::::::::::

### Calculating mean and standard deviation

```{r}
mean(hgtwgt_survey$weight.kg., na.rm = TRUE)
```

Calculate the standard deviation and confirm that it is the square root of the variance:

```{r}
sdweight <- sd(hgtwgt_survey$weight.kg., na.rm = TRUE)
print(sdweight)

varweight <- var(hgtwgt_survey$weight.kg., na.rm = TRUE)
print(varweight)
sqrt(varweight) == sdweight
```

The `na.rm` argument tells R to ignore missing values in the variable.

### Calculating median and interquartile range

```{r}
median(hgtwgt_survey$weight.kg., na.rm = TRUE)
```

```{r}
IQR(hgtwgt_survey$weight.kg., na.rm = TRUE)
```

Again, we ignore the missing values.

## Describing discrete data

In our data set there is a variable `gender.M.`, where there is a 1 this represents a Male, when there is a 0 this represents a Female. What is the proportion of males and females in this data set?

-   Frequencies

```{r}
table(hgtwgt_survey$gender.M.)
```

-   Proportions

```{r}
gendertable <- table(hgtwgt_survey$gender.M.)
prop.table(gendertable)
```

Contingency tables of frequencies can also be tabulated with **table()**. For example:

```{r}
table(
  hgtwgt_survey$gender.M.,
  hgtwgt_survey$age.yrs.
)
```

Which leads quite naturally to the consideration of any association between the observed frequencies.

# Inferential statistics

## Meaningful analysis

-   What is your hypothesis - what is your null hypothesis?

::: callout
Always: the level of the independent variable has no effect on the level of the dependent variable.
:::

-   What type of variables (data type) do you have?

-   What are the assumptions of the test you are using?

-   Interpreting the result

## Testing significance

-   p-value

-   \<0.05

-   0.03-0.049

    -   Would benefit from further testing.

**0.05** is not a magic number.

## Comparing means

It all starts with a hypothesis

-   Null hypothesis
    -   "There is no difference in mean height between men and women" $$mean\_height\_men - mean\_height\_women = 0$$
-   Alternate hypothesis
    -   "There is a difference in mean height between men and women"

## More on hypothesis testing

-   The null hypothesis (H0) assumes that the true mean difference (μd) is equal to zero.

-   The two-tailed alternative hypothesis (H1) assumes that μd is not equal to zero.

-   The upper-tailed alternative hypothesis (H1) assumes that μd is greater than zero.

-   The lower-tailed alternative hypothesis (H1) assumes that μd is less than zero.

-   Remember: hypotheses are never about data, they are about the processes which produce the data. The value of μd is unknown. The goal of hypothesis testing is to determine the hypothesis (null or alternative) with which the data are more consistent.

## Comparing means

Let's use the hypothesis introduced aboove: is there is a difference in mean height between men and women?

```{r}
hgtwgt_survey %>%
  group_by(gender.M.) %>%
  summarise(mean = mean(height.cm., na.rm=TRUE), n = n())
```


Is the difference between the income ranks statistically significant?

## t-test

### Assumptions of a t-test

-   One independent categorical variable with 2 groups and one dependent continuous variable

-   The dependent variable is approximately normally distributed in each group

-   The observations are independent of each other

-   For students' original t-statistic, that the variances in both groups are more or less equal. This constraint should probably be abandoned in favour of always using a conservative test.

## Doing a t-test

```{r}
t.test(height.cm. ~ gender.M., data = hgtwgt_survey)

# we can also specify specific components
t.test(height.cm. ~ gender.M., data = hgtwgt_survey)$statistic
t.test(height.cm. ~ gender.M., data = hgtwgt_survey)$parameter
```

Notice that the summary()** of the test contains more data than is output by default.

### t-test result

Testing supported the rejection of the null hypothesis that there is no difference between the height of males and females in this data set (**t**=`r round(t.test(height.cm. ~ gender.M., data = hgtwgt_survey)$statistic,4)`, **df**= `r round(t.test(height.cm. ~ gender.M., data = hgtwgt_survey)$parameter,4)`,
**p**= `r round( t.test(height.cm. ~ gender.M., data = hgtwgt_survey)$p.value,4)`).

(Can you get p to display to four places?  Cf *format()*.)

## More than two levels of IV

While the t-test is sufficient where there are two levels of the IV, for situations where there are more than two, we use the **ANOVA** family of procedures. To show this, we will compare the `height.cm.` between `age.yrs` . If the ANOVA result is statistically significant, we will use a post-hoc test method to do pairwise comparisons (here Tukey's Honest Significant Differences.)

```{r}
anovamodel <- aov(hgtwgt_survey$height.cm. ~ hgtwgt_survey$age.yrs.)
summary(anovamodel)

TukeyHSD(anovamodel)
```

# Regression Modelling

The most common use of regression modelling is to explore the relationship between two continuous variables, for example between `weight.kg.` and `height.cm.` in our data. We can first determine whether there is any significant correlation between the values, and if there is, plot the relationship.

```{r}
cor.test(hgtwgt_survey$weight.kg., hgtwgt_survey$height.cm.)

ggplot(hgtwgt_survey, aes(weight.kg., height.cm.)) +
  geom_point() +
  geom_smooth()
```

Having decided that a further investigation of this relationship is worthwhile, we can create a linear model with the function `lm()`.

``` {r}
modelone <- lm(hgtwgt_survey$weight.kg. ~ hgtwgt_survey$height.cm.)
summary(modelone)
```

## Regression with a categorical IV (the t-test)

Run the following code chunk and compare the results to the t-test conducted earlier.

```{r}
hgtwgt_survey %>%
  mutate(gender.M. = factor(gender.M.))

modelttest <- lm(hgtwgt_survey$height.cm. ~ hgtwgt_survey$gender.M.)

summary(modelttest)
```
:::::::::::::::::::::::::::::::::::::::  challenge

## Challenge: Regression with a categorical IV (ANOVA)

Use the `lm()` function to model the relationship between `hgtwgtsurvey$height.cm.`
and `hgtwgtsurvey$age.yrs.`. 

Compare the results with the ANOVA carried out earlier.

:::::::::::::::  solution

## Solution

First we need to convert `age.yrs.` to a factor, then we can create our model.
If we compare the p-values for the Anova (0.908) and the lm we have just created (0.9083)
we can see that the outcome is the same.

```{r}
hgtwgt_survey %>%
  mutate(age.yrs. = factor(age.yrs.))

modelttest <- lm(hgtwgt_survey$height.cm. ~ hgtwgt_survey$age.yrs.)

summary(modelttest)

```

:::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::::::::::::::::




:::::::::::::::::::::::::::::::::::::::: keypoints

- R has a range of in-built functions to enable initial data exploration.
- Linear models (lm) can be used with continuous and categorical variables.

::::::::::::::::::::::::::::::::::::::::::::::::::
